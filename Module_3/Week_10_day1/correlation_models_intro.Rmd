---
title: "Correlation, linerar_models, RESGRESSION_intro"
output: html_notebook
---
# Correlation
## !does not imply causation!
```{r}
library(tidyverse)
library(janitor)
library(modelr)
library(broom)
library(ggfortify)
```

```{r}
mtcars %>%
  select(wt, mpg) %>%
  glimpse()
```
```{r}
mtcars %>%
  ggplot(aes(x = wt, y = mpg)) +
  geom_point()
```
```{r}
noisy_bivariate <- function(noise = 1, gradient = 1){
  x <- runif(n = 200, min = 0, max = 10)
  y <- gradient * x + 10
  y_scatter <- noise * 4 * rnorm(n = 200)
  y <- y + y_scatter
  data = tibble(x, y)
  r <- round(cor(x, y), 4)
  title <- paste(
    "noise = ", noise,
    ", gradient = ", gradient,
    ", r = ", r
  )
  data %>%
    ggplot(aes(x = x, y = y)) +
    geom_point() +
    xlim(0, 10) +
    ylim(min(c(min(y), 0)), max(c(max(y), 10))) +
    ggtitle(title)
}
noisy_bivariate(noise = 0.3, gradient = 1)
```
```{r}
#summary(state.x77)
tibble_states <- clean_names(as_tibble(state.x77))
#View(tibble_states)
correlation_value <- paste("correlation value = ", round(digits = 2, cor(tibble_states$population, tibble_states$income)))

```
```{r}
tibble_states %>%
  ggplot(aes(x = population, y = income)) +
  geom_point()+
  ggtitle(correlation_value)
```

# Models 

y = mx + c (m = gradient, c = intercept) y = ax+b

gradient = change(y) / change(x)

linear regression is about gradients :) 

```{r}
line <- function(x, a, b){
  # a is gradient, b is intercept
  return(a * x + b)
}
```
```{r}
data <- tibble(
  x = seq(-5, 5, 0.1),
  y = line(x, a = -1, b = -1),
  y2 = line(x, a = 1, b = -2),
  y3 = line(x, a = 3, b = 0.5)
)
```
```{r}
data %>%
  ggplot(aes(x, y))+
  geom_line(color = "red") +
  geom_line(aes(x, y2), color = "green") +
  geom_line(aes(x, y3), color = "blue") +
  geom_vline(xintercept = 0) +
  geom_hline(yintercept = 0) +
  coord_fixed()
```
## Centroid:

a point with coordinates of (mean of x, mean of y)
```{r}
noisy_line <- read_csv("data/noisy_line.csv")

noisy_line_plot <- noisy_line %>%
  ggplot(aes(x, y)) + 
  geom_point()
noisy_line_plot
```
```{r}
centroid <- noisy_line %>%
  summarise(
    x = mean(x),
    y = mean(y)
  )
centroid
```
```{r}
noisy_line_plot <- noisy_line_plot +
  geom_point(aes(x = centroid$x, y = centroid$y), col = "red", size = 5)
noisy_line_plot
```

y = ax+b
(slope)b = y-ax

```{r}
get_intercept <- function(slope, centroid_x, centroid_y){
  #when fitting the line we need to choose our slope
  return(centroid_y - slope * centroid_x)
}
```
```{r}
#done by vsual estimate - not ideal, best to use geom_smooth
slope = 2.25
noisy_line_plot+
  geom_abline(slope = slope, intercept = get_intercept(slope, centroid$x, centroid$y))

noisy_line_plot +
  geom_smooth(method = "lm", se = FALSE)
```
Quadritic parabola or other polynominals could be fitted to when the data is not very linear, i.e. a line fits at the start, but not the end and vice versa.

# REGRESSION

single explanatory variable = x

single outcome variable = y

Simple linear regression (single x)

---

weight: explanatory variable (x)

height: outcome variable (y)

Regression: describe the relationship, make predictions

```{r}
height <- c(176, 164, 181, 168, 195, 185, 166, 180, 188, 174)
weight <- c(82, 65, 85, 76, 90, 92, 68, 83, 94, 74 )
sample <- tibble(
  weight,
  height
)
sample %>%
  ggplot(aes(x = weight, y = height)) +
  geom_point()
```
```{r}
line <- function(x, b0, b1){
  return(b0 + b1 * x)
}
```
```{r}
sample <- sample %>%
  mutate(fit_height = line(x = weight, b0 = 95, b1 = 1))
```
```{r}
plot <- sample %>%
  ggplot(aes(x = weight, y = height)) +
  geom_point() +
  geom_point(aes(y = fit_height), shape = 1) +
  geom_abline(slope = 1, intercept = 95, col = "red") +
  geom_segment(aes(xend = weight, yend = fit_height), alpha = 0.5)
plot
```
```{r}
sample <- sample %>%
  mutate(residual = height - fit_height)

sample %>%
  summarise(
    sum_sq_residuals = sum(residual^2)
    )
```
Least squares regression: sum_sq_residuals as small as possible

Least abs value regression: sum_sq_residuals = abs(residual)

above is all manual method, do not do use it!!!

Now same but using R

```{r}
model = lm(formula = height ~ weight, data = sample)
model
```
This is called "Patsy" formula notation

From above output we can infer that predicted 

height = 102.1758(intercept) + 0.9336(weight) * weight 

```{r}
fitted(model) #for existing data points
```

What height does the model predict for someone who weighs 78kg?
```{r}
#base R method
predict_at <- data.frame(weight = c(78))
predict(model, newdata = predict_at)
```
```{r}
sample <- sample %>%
  select(weight, height)
sample
```
```{r}
#tidy method
sample <- sample %>%
  add_predictions(model) %>%
  add_residuals(model)

sample
```
```{r}
sample %>%
  ggplot(aes(x = weight))+
  geom_point(aes(y = height))+
  geom_line(aes(y = pred),color = "red")
```
```{r}
#predicting data not in original data
weights_predict <- tibble(weight = 50:120)

weights_predict %>%
  add_predictions(model)
```

How do we interpret the relationship (slope)?

Slope b1, b_weight = 0.9336: is slope in height/weight plot (for each 1kg in weight height changes by 0.9336cm)

# Regression diagnostics

```{r}
model <- lm(height ~ weight, data = sample)
model
```
```{r}
summary(model)
tidy_output <- clean_names(tidy(model))
glance_output <- clean_names(glance(model)) # model performance data
```

how well does the line fit the data? (goodness - of - fit)
r_squared aka coefficient of determination.
```{r}
#r squared value
glance_output$r_squared
```
this means that 85.3% of variation in height in the dataset can be explained by the variation in weight.

r_squared = 1 - is ideal fit

r_squared = 0 - model does not fit

```{r}
r <- cor(sample$height, sample$weight)
r^2
tidy_output
```

In order to be reliable, p - values have o be "normal".

```{r}
autoplot(model)
```

Residuals vs Fitted - by how much the line missed each data point. Want to see them scattered near zero (evenly spread above and below zero)

Normal Q-Q - lets to see if points are normally distrubuted - points should be close to line

Scale-Location - ideally poits should have "horizontal trend"

If any of the autoplots are not satisfactory, need to think about BOOTSTRAPPING the regression.


# Task

```{r}
distribution_1 <- read_csv("data/distribution_1.csv")
distribution_2 <- read_csv("data/distribution_2.csv")
```
```{r}
dist_1_model <- lm(y ~ x, data = distribution_1)
autoplot(dist_1_model)
```

```{r}
dist_2_model <- lm(y ~ x, data = distribution_2)
autoplot(dist_2_model)
```
```{r}
distribution_1 <- distribution_1 %>%
  add_predictions(dist_1_model)

distribution_2 <- distribution_2 %>%
  add_predictions(dist_2_model)

distribution_1 %>%
  ggplot(aes(x = x))+
  geom_point(aes(y = y))+
  geom_line(aes(y = pred), color = "red")+
  ggtitle("distribution_1")

distribution_2 %>%
  ggplot(aes(x = x))+
  geom_point(aes(y = y))+
  geom_line(aes(y = pred), color = "red")
```



